---
title: "Work Attrition Analysis - A Data Science Project"
author: "Stefan Anca"
date: "2024-08-13"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 - INTRODUCTION.

## 1.1 - R Markdown.

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. In this project I will keep available this Markdown file, and also the result from exporting it to HTML. The markdown file will have a plain text format, but the HTML file will have a nice formatted aspect.

## 1.2 - Goal of the project.

The objective of this project will be to describe and explain how job abandonment occurs. Specifically, associations between different variables and job abandonment will be sought in order to describe when this occurs and to establish possible explanations for the phenomenon. Multiple techniques will be used for variable manipulation, data cleaning, data exploration, and clustering models to try to identify specific groups. The main goal is to explore the functioning of the algorithms and demonstrate ways of working with data at the time that we attempt a specific goal, rather than creating the best model. For this secondary objective, multiple iterations of different actions performed in this script may be necessary.


## 1.3 - Dataset description.

The selected dataset, which comes from the field of human resource management, is named "Employee Attrition and Factors". Each row corresponds to a specific employee of the company and includes some demographic variables such as age and education level (among others) containing additionally multiple variables describing the job position, such as the frequency of work-related travel, the department in which the employee works, and the distance between the workplace and the employee's home. Therefore, this dataset provides data that allows for the analysis of job abandonment from both a professional and personal perspective, making it ideal for the initial objective of describing and explaining job abandonment.

The dataset is available in Kaeggle: https://www.kaggle.com/datasets/thedevastator/employee-attrition-and-factors

The name and meaning/description of the variables are presented below:


**Age**: The age of the employee. (Numerical)

**Attrition**: Whether or not the employee has left the organization. (Categorical)

**BusinessTravel**: The frequency of business travel for the employee. (Categorical)
  
**DailyRate**: The daily rate of pay for the employee. (Numerical)

**Department**: The department the employee works in. (Categorical)

**DistanceFromHome**: The distance from home in miles for the employee. (Numerical)

**Education**: The level of education achieved by the employee. (Categorical)
    
**EducationField**: The field of study for the employee's education. (Categorical)

**EmployeeCount**: The total number of employees in the organization. (Numerical)
    
**EmployeeNumber**: A unique identifier for each employee profile. (Numerical)

**EnvironmentSatisfaction**: The employee's satisfaction with their work environment. (Categorical)
    
**Gender**: The gender of the employee. (Categorical)

**HourlyRate**: The hourly rate of pay for the employee. (Numerical)
    
**JobInvolvement**: The level of involvement required for the employee's job. (Categorical)
    
**JobLevel**: The job level of the employee. (Categorical)
  
**JobRole**: The role of the employee in the organization. (Categorical)
    
**JobSatisfaction**: The employee's satisfaction with their job. (Categorical)
    
**MaritalStatus**: The marital status of the employee. (Categorical)
    
**MonthlyIncome**: The monthly income of the employee. (Numerical)
    
**MonthlyRate**: The monthly rate of pay for the employee. (Numerical)
  
**NumCompaniesWorked**: The number of companies the employee has worked for. (Numerical)
  
**Over18**: Whether or not the employee is over 18. (Categorical)

**OverTime**: Whether or not the employee works overtime. (Categorical)

**PercentSalaryHike**: The percentage of salary hike for the employee. (Numerical)

**PerformanceRating**: The performance rating of the employee. (Categorical)

**RelationshipSatisfaction**: The employee's satisfaction with their relationships. (Categorical)
    
**StandardHours**: The standard hours of work for the employee. (Numerical)

**StockOptionLevel**: The stock option level of the employee. (Numerical)

**TotalWorkingYears**: The total number of years the employee has worked. (Numerical)

**TrainingTimesLastYear**: The number of times the employee was taken for training in the last year. (Numerical)
    
**WorkLifeBalance**: The employee's perception of their work-life balance. (Categorical)

**YearsAtCompany**: The number of years the employee has been with the company. (Numerical)
    
**YearsInCurrentRole**: The number of years the employee has been in their current role. (Numerical)

**YearsSinceLastPromotion**: The number of years since the employee's last promotion. (Numerical)
    
**YearsWithCurrManager**: The number of years the employee has been with their current manager. (Numerical)

# 2. EXPLORATORY DATA ANALYSIS (EDA).

## 2.1 - Number of variables of each type.

```{r}
# Load the data
HR_data <- read.csv("WokingAttrition.csv")

# Show how many variables of each class there are
barplot(table(sapply(HR_data, class)), col = "aquamarine")
```

## 2.2 - Analysing character variables.

Since we have multiple variables in the dataset, we will try to make the exploratory analysis more manageable. First, we will focus on the character variables, and after on the numeric variables.

```{r}
# Using "sapply" we get a vector showing which columns are character
char_columns <- sapply(HR_data, class) == "character"
# We save in a dataframe the character variables
character_var <- HR_data[char_columns]
# We show their names
colnames(character_var)
```

```{r}
# We show the number of unique values of each variable
sapply(character_var, function(x) length(unique(x)))
```

### 2.1.1 - Character variables with 1 cathegory.

Only observing the number of unique values we can observe some interesting things. First, "Over18" has only 1 unique value. In addition, there are multiple variables with only 2 or 3 unique values, and just 2 variable have more than 3 unique values. We will explore this variables starting from those that have less unique values to those that have more.

```{r}
# First we will show the unique value of the variable "Over18"
unique(HR_data$Over18)
```

We can see that the unique value is "Y". This indicates that all individuals are adults. In the data cleaning process, we will discard this variable, as is a constant that we already know from this point about the dataset.

### 2.1.2 - Character variables with 2 or 3 cathegories.

Now, we will make a graphical representation of the variables with 2 and 3 unique values.

```{r, warning=FALSE}
# We load some packages necesary for the visualisation and data manipulation
library(ggplot2)
library(dplyr)
library(tidyr)
# We save in a data frame the variable with 2 or 3 unique values
data_2_3_unique <- HR_data[c("Attrition", "Gender", "OverTime", "BusinessTravel", "Department", "MaritalStatus")]

# We transform this data to a long format
data_long <- data_2_3_unique %>% 
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Category") %>%
  count(Variable, Category) %>% #We count how many times appear each category of each variable
  group_by(Variable) %>% # We group by variable
  mutate(Prop=n/sum(n)) # We calculate the proportion of each category

# We show the result
data_long
```

**GRAPHICAL REPRESENTATION OF VARIABLES WITH 2 AND 3 CATHEGORIES.**

Now we are going to represent this data graphicaly. On the x-axis, will be included each variable, and on the y-axis, the height represents the proportion of each category. For each variable, we will fill the figure based on the proportion that each category appears within the variable. We will create a bar chart using the values displayed in the dataframe.

```{r fig.width=12, fig.height=12}
# Initialization of ggplot graph
ggplot(data_long, aes(x = Variable, y = Prop, fill = Category)) +
  geom_bar(stat = "identity") +
  # Text is added to axes, title, and for each category represented
  geom_text(aes(label = paste0(Category, "\n", round(Prop * 100, 1), "%")), 
            position = position_stack(vjust = 0.6), size=4) +
  labs(y = "Proportion", title = "Proportion of values of the variables with 2 or 3 unique values") +
  # The legend text will have an angle
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 20, hjust = 1, size = 15),
  )
```

In this chart we can observe several things. First, for binary variables, in all cases, one of the variables is somewhat or significantly more dominant than the others (the proportion is not close to 50/50 in any case). On the other hand, regarding the variables with 3 categories, several aspects can also be considered. First, in the 'Department' variable, there are only 3 unique values. This indicates that the sample is not representative of all types of jobs that might exist, but rather that it likely comes from a specific sector (since most of the workers in the sample are in the Research and Development department). Additionally, the 'Human Resources' category is underrepresented (only 4.3% of the cases). Regarding 'BusinessTravel,' we see a clear predominance of people who rarely travel over those who travel frequently or do not travel at all. Finally, concerning the 'MaritalStatus' variable, there is some balance between the proportions of the 3 categories, with only a slight predominance of the 'Married' category.

### 2.1.3 - Other character variables.

After analyzing the character variables with little unique values, we will analyse the others character variables.

```{r}
# We save in a dataframe the character variables that we didn't analysed
rest_character_var <- HR_data[c("EducationField" ,"JobRole")]

# We will se a frequency table for each variable
data.frame(table(rest_character_var[1]))
data.frame(table(rest_character_var[2]))
```

We see that for both variables, the categories are limited to a really speciffic field. It seems as though this data is related to the healthcare sector, and includes some complementary departments like Marketing or Human Resources. The number of employees in each category varies significantly across all categories.

## 2.2 - Cathegoric variables stored as numeric.

Once the analysis with the variables stored as "character" in the dataset is complete, we will start analyzing the numeric variables. First we will analyze some variables that were marked as categorical in the previously created variable dictionary, but which we did not find among the character class variables, specifically:

Education, EnvironmentSatisfaction, JobInvolvement, JobLevel, JobSatisfaction.

```{r}
# We create a vector of characters containing the name of these variables
names <- c("Education", "EnvironmentSatisfaction", "JobInvolvement", "JobLevel", "JobSatisfaction")
# We create a data frame which includes only these variables using the vector
df_aux <- HR_data[names]

# We observe the structure of this data frame
str(df_aux)
```

We see that in all these cases, these are variables whose values are encoded as numbers (integers), but on the download page, they are recorded as categorical. Therefore, the numbers probably represent categories whose values we cannot necessarily treat as continuous. Given the names, descriptions and data type (integer) of the variables, we should probably interpret these values as ordinal. In other words, with these variables, we can determine in which observation there are higher levels of a certain variable, but we cannot make arithmetic comparisons between the different levels (for example, we cannot say that 2 represents double of 1 in a speccific variable, we only knwo that 2 is higher than 1).

We will create an histogram in order to understand how this variables are.

```{r}
par(mfrow = c(1,2))
# We iterate over the names of the auxiliar data frame previously created
for (var in colnames(df_aux)) {
  # An histogram is created for each iterated variable
  hist(df_aux[[var]], main = var)
}
```

If we look at the histogram, we find very limited ranges for these variables (which is typical in ordinal variables). For all the values of the variables, there are bars with a minimum number of observations so that they are identifiable. The only issue we could point out is that since there are no labels showing the meaning of each value, we cannot distinguish whether the maximum and minimum values are the theoretical of the variable, or just the maximums/minimums within this sample. It could be the case that these variables could have been scored above or below the maximums/minimums shown, but this values weren't in this sample. Since the download page doesn't provide clarifications about the range these variables can take, we can't be certain about this. To address this issue, when we will prepare the data, we will create some variables from these to allow us to establish clear differences between the subjects if we use these variables.

## 2.3 - Other numeric variables.

### 2.3.1 - General exploration.

Now we will analyse the rest of the numeric variables.

```{r}
# We create a dataframe with only the numeric variables
numeric_var <- HR_data[sapply(HR_data, FUN=is.numeric)]
# We exclude the ones that were truly categorical but were marked as numeric.
indices_to_eliminate <- match(names, names(numeric_var))
numeric_var <- numeric_var[-indices_to_eliminate]

# We make a summary of these variables
summary(numeric_var)
```

There are several things we can observe in this summary; let's start handling them one by one.

**- Variables with the same value in all observations:**

There are two cases where this occurs: EmployeeCount and StandardHours.

In the data cleaning step of this data mining process, we will discard these variables since they are not useful for model development (because there is no variability in their values, making it impossible to distinguish cases using these variables).

**- Other variables that also seem ordinal rather than quantitative:**

The variables shown in this summary that appear to be ordinal are: RelationshipSatisfaction, WorkLifeBalance, PerformanceRating, StockOptionLevel.

In all these cases, the maximum and minimum values are between 0 and 4. Additionally, all summary statistics, except for the mean, are integers, and if we consider their names and definitions, in none of the cases can the values be interpreted as a specific quantity of a magnitude (the names do not refer to specific physical objects that can be measured directly). This leads us to suspect that their values are indeed ordinal.

### 2.3.2 - Variables that seem cathegoric, but are stored and described as numeric.

Next, we will display histograms of all these variables to see their distributions.

```{r}
par(mfrow=c(1,2))
# We save in a vector the names of these variables
ordinal_names <- c("RelationshipSatisfaction", "WorkLifeBalance", "PerformanceRating", "StockOptionLevel")
# We iterate over these names
for (name in ordinal_names) {
  # For each name, an histogram is created with the column of the dataset with tha same name
  hist(numeric_var[[name]], main = name)
}
# We also get the number of unique values of each variable
sapply(numeric_var[ordinal_names], function(x) length(unique(x)))
```

We can see that their values are discrete since each one has the same number of unique values as bars in the histogram. They seem to be values from a questionnaire or some index with a score that is always an integer. Therefore, we encounter the same issue as before: we don't know if the minimum and maximum observed in this sample are the true minimum and maximum values that these variables can take, or if they could actually have higher or lower values. Additionally, we do not know the exact meaning of each level of the categories of these variables. Therefore, when using these variables in analyses, we must be cautious about the specific interpretations made about the meaning of their magnitudes.

### 2.3.3 - Univariate outliers of numeric continuous variables.

The rest of the quantitative variables do not seem to have any peculiarities that would lead us to think they should be excluded or that they are actually categorical. Let's analyze them graphically.

```{r}
# We create a vector with tha names of all the cuantitative variables that we already observed.
analysed_variables <- c("EmployeeCount","StandardHours","EmployeeNumber", ordinal_names)

# We save in a vector the indexes of this columns in the "numeric_var" data frame
observed_indexes <- match(analysed_variables, names(numeric_var))

# We create other data frame which will not include these variables
not_observed_numeric <- numeric_var[-observed_indexes]

# We will visualize these variables using boxplots

# We will show them 5 by 5
par(mfrow=c(1,5))
# We iterate over tha names of the columns
for (var in colnames(not_observed_numeric)) {
  # And create a boxplot for each variable
  boxplot(not_observed_numeric[[var]], main = var)
}
```

In these boxplots, we can observe several things. First, some of them are very similar to a normal distribution, as the median is right in the center of the figure, and the rest of the figure is symmetric above and below the median. On the other hand, many of these variables do not have univariate outliers, as there are no points appearing above or below the interquartile range. This could be positive since they won’t interfere with the models we train. However, there are also other boxplot that show univariate outliers.

Therefore, we will explore how many outliers are present in each variable.

```{r}
sapply(not_observed_numeric, function(x) length(boxplot.stats(x)$out))
```

We observe that there are indeed variables without outliers. On the other hand, when outliers are present, they are quite few compared to the total number of observations (1470). Only one of the variables has more than 10% outliers (TrainingTimesLastYear).

Another aspect to observe is that in all the variables involving the temporal dimension (those that have 'Year' in their name), there are outliers, what raise suspicions that these outliers might be related to each other. We will display a correlation matrix of the quantitative variables to understand the relationships between all variables, including the relationships between the temporal variables.

### 2.3.4 - Correlations between numeric continuous variables.

```{r, warning=FALSE}
# Import ggcorrplot
library(ggcorrplot)
# We create and visualize a correlation matrix with the numeric variables that we know that are quantitative
cor_numerics <- cor(not_observed_numeric)
ggcorrplot(cor_numerics)
```

There are several insights in this correlation matrix. First, all the variables that represent a temporal dimension are highly related to each other, which strengthens the previous idea that their outliers could be related. Additionally, there are other variables that correlate with the temporal ones, specifically age and monthly salary. The rest of the variables, as we can see, do not show any type of relationship. It is curious to note that the variable 'TrainingTimesLastYear' is not correlated with any variable, and in this case, it was the variable with the highest number of outliers.

### 2.3.5 - Multivariate outliers and their relation with univariates.

We will now perform a multivariate analysis of the outliers in the variables that we have observed to be related. If there exist multivariate outliers, we will also verify if these outliers are at the same time univariate outliers. This will help us to better understand the composition of those.

```{r, warning=FALSE}
# We save in a vector these variables
names_high_cor <- c("YearsWithCurrManager", "YearsSinceLastPromotion", "YearsInCurrentRole", "YearsAtCompany", "TotalWorkingYears", "Age")
# We select these variables
vars_high_cor <- HR_data[names_high_cor]
```

For studying multivariate outliers, we will use the mahalanobis_distance function from the rstatix package to calculate the Mahalanobis distance from each point to the center of the data, and mark all values that represent outliers according to this distance.

```{r, warning=FALSE}
library("rstatix")
vars_high_cor <- mahalanobis_distance(vars_high_cor)

# We eliminate the column that indicates the mahalanobis distance of each point to the center
vars_high_cor$mahal.dist <- NULL
# Now we will create a function that will replace each variable with a column where TRUE will indicate if for that variable the observation is a univariate outlier, and FALSE will indicate that is not
determine_univariate_outliers <- function(column) {

# We get the values of the first and the third quartile
Q1 <- quantile(column, 0.25, na.rm = TRUE)
Q3 <- quantile(column, 0.75, na.rm = TRUE)
# Using this values we calculate the interquartile range
IQR <- Q3 - Q1
# We create the limits of what is considered an outliers or not (Using the general aproximation of +- 1.5IQR)
bottom_limit <- Q1 - 1.5 * IQR
top_limit <- Q3 + 1.5 * IQR

# We determine which observation is an outlier, and generate a TRUE value when is it, and a FALSE value when is not
column <- column < bottom_limit | column > top_limit
return(column)}

# We will use this function for each column of the previous saved variables
for(name in names_high_cor) {
  vars_high_cor[[name]] <- determine_univariate_outliers(vars_high_cor[[name]])
}

# We visualize the aspect of this dataframe
head(vars_high_cor)
```

After this process, we have obtained a dataframe where each observation is marked with a boolean value in each column. In the columns named as the variables, the boolean values indicates whether the observation is an outlier or not, and in the last column, the boolean indicates whether the observation is an outlier considering the entire dataset as a whole.

```{r}
# We show how many outliers there are in each column
colSums(vars_high_cor)
```

We can see that in some cases the number of univariate outliers is higher than the number of multivariate, and in others is smaller. Now we will verify what happens if we only observe the cases where the observations are mulitivariate outliers.

```{r}
colSums(vars_high_cor[vars_high_cor["is.outlier"]==TRUE,])
```

We can see that in some variables, most of the multivariate outliers are at the same time univariate. However, in other cases, this is not happening (for example with the variable "YearsInCurrentRole", only 5 observations are univariate outliers). So we can conclude that some of the outliers of these variables are related, but not most of them (because there are a lot more univariate outliers in some cases that multivariate).

## 2.4 - Missing data.

Now we will verify how many missing values are in the dataset.


```{r}
sum(is.na(HR_data))
sum(HR_data==" ")
```

We can see that there are not missing values in this dataset. This is something positive, because we will not generate biases using imputation methods.

# 3 - DATA CLEANING.

## 3.1 - Columns with only 1 unique value.

In the previously conducted EDA, we observed that multiple columns had the same and unique value. We will eliminate these columns.

```{r}
# We save in a vector the name of these variables
constants_names <- c("EmployeeCount", "StandardHours")
# We search the indexes of the columns with these names
indexes_to_eliminate <- match(constants_names, names(HR_data))
# We exclude the columns with these indexes from the complete dataset
HR_data <- HR_data[-indexes_to_eliminate]

# We will also discard the identifier of the employees, because is not necesary for the analysis
HR_data <- HR_data[-(match("EmployeeNumber", names(HR_data)))]
```

## 3.2 - Modifying some data types.

We are going to change the values of the binary variables from Yes/No to 1 or 0 (which will convert the variables to numeric).

```{r}
HR_data$Attrition <- ifelse(HR_data$Attrition=="No", 0, 1)
HR_data$OverTime <- ifelse(HR_data$OverTime=="No", 0, 1)
```

Now, we are going to convert the remaining character variables to factor type.

```{r}
# The levels of the variable "BusinessTravel" can be sorted, we will make this conversion considering their levels
HR_data$BusinessTravel <- factor(HR_data$BusinessTravel, levels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently"))

# Now we will convert the rest of the character variables
# We create a vector with their names
factor_var <- c("Department", "Gender", "MaritalStatus", "EducationField", "JobRole")

# And we make the conversion
HR_data[factor_var] <- lapply(HR_data[factor_var], as.factor)
```

## 3.3 - Codification of new variables.

Some of the variables we've identified as ordinal are those whose names end in "Satisfaction". For these variables, we mentioned that we don't have information about their actual range, as the dataset download link didn't provide the theoretical range for these scales. This, as we've argued, could imply that these variables might theoretically have values higher than those that are present in this sample. Therefore, if we use the values as they appear in this dataset, we won't be able to interpret them correctly (since we won't know the real meaning of each value, we can only order them without knowing if the values actually represent a high or low magnitude).

However, there is something we can do with these variables to give them a bit more interpretability. We can distinguish between the people with the highest satisfaction in the entire sample and those with the lowest satisfaction. To identify which individuals are the most and least satisfied, we will set conditions for the variables that measure satisfaction. We will create two binary variables whose values will be determined in both cases by a condition:

**Variable 1:** 'Max_satisfaction' will take the value "1" when at least two of the satisfaction variables (there are only three in total) have the highest value in the sample for that variable, and 0 when they do not.

**Variable 2:** 'Min_satisfaction' will take the 1 when at least two of the satisfaction variables have the lowest value in the sample for that variable, and 0 when they do not.

Next, we will create these variables:

```{r}
# High satisfaction
HR_data$High_satisfaction <- ifelse(
  (HR_data$JobSatisfaction == max(HR_data$JobSatisfaction)) +
  (HR_data$EnvironmentSatisfaction == max(HR_data$EnvironmentSatisfaction)) +
  (HR_data$RelationshipSatisfaction == max(HR_data$RelationshipSatisfaction)) >= 2,
                                    1, 0)
# Low satisfaction
HR_data$Low_satisfaction <- ifelse(
  (HR_data$JobSatisfaction == min(HR_data$JobSatisfaction)) +
  (HR_data$EnvironmentSatisfaction == min(HR_data$EnvironmentSatisfaction)) +
  (HR_data$RelationshipSatisfaction == min(HR_data$RelationshipSatisfaction)) >= 2,
                                   1, 0)

# We also transform other some numeric variables to factor, including those that we have just created (we use the same procedure as before, but with other variables. We do it separately for having more modularity in the transformation of both types of variables)
factor_var <- c("Attrition", "Education", "EnvironmentSatisfaction", "JobInvolvement", "JobSatisfaction", "OverTime", "PerformanceRating", "RelationshipSatisfaction", "StockOptionLevel", "WorkLifeBalance", "High_satisfaction", "Low_satisfaction")
HR_data[factor_var] <- lapply(HR_data[factor_var], as.factor)
```

## 3.4 - Outliers handling.

For handling them, we will make some tests, in order to find a good solution. We will make some transformations, and verify their effects.

```{r}
# We create a vector with the names of the variables that we identified in the previous section that have outliers.
vars_outliers <- c("MonthlyIncome", "NumCompaniesWorked", "TotalWorkingYears", "TrainingTimesLastYear",  "YearsAtCompany", "YearsInCurrentRole", "YearsSinceLastPromotion","YearsWithCurrManager")

# We create a version of these variables trasnformed using square root
for (name in vars_outliers) {

# We will create a new name to which we add at the end of the iterated name, "_scaled". We use this name because we are going to standardize it later (the idea is to first transform with the square root to reduce the effect of outliers, and then standardize).
  name_scaled <- paste0(name, "_scaled")
# We create a new variable, that contains the square root of the iterated variable and has the new created name
  HR_data[[name_scaled]] <- sqrt(HR_data[[name]])
}
# After transforming the variable, we will verify how many outliers are in the variable
for (name in vars_outliers) {
  name_scaled <- paste0(name, "_scaled")
  print(name_scaled)
  print(length(boxplot.stats((HR_data[[name_scaled]]))$out))
}
```

We can see that now only 3 variables still have outliers. Moreover, in 2 of these, the number of outliers is lower than it was before, indicating that the transformation reduces the number of outliers or eliminates them completely. The last step is to check how far the remaining outliers are from the center of the data in terms of standard deviations, compared to the original variables.

```{r}
# We create a vector in which we store the names of the variables transformed using the square root where still remain outliers.
still_outliers <- c("TotalWorkingYears", "TrainingTimesLastYear", "YearsAtCompany")


# We create a function that calculates the distance in standard deviations from the highest outlier and the mean
distancia_outliers <- function(column) {
  # We will create the name of the transformed variables adding "_scaled" at the end
  scaled_name <- paste0(column, "_scaled")
  
  # We get the mean and the standart deviation of the original variablo and the transformed
  sd_normal <- sd(HR_data[[column]])
  mean_normal <- mean(HR_data[[column]])
  sd_scaled <- sd(HR_data[[scaled_name]])
  mean_scaled <- mean(HR_data[[scaled_name]])

  # We get the outliers from the original variable and the transformed one
  outliers_normal <- boxplot.stats(HR_data[[column]])$out
  outliers_scaled <- boxplot.stats(HR_data[[scaled_name]])$out

  # We calculate the distance in standard deviations for the outlier with the highest value in the original variable
  print(column)
    print(max((outliers_normal - mean_normal) / sd_normal))
    
# We repeat the process with the transformed variable 
  print(scaled_name)
    print(max((outliers_scaled - mean_scaled) / sd_scaled))
}

# We use this function with all the variables where remain outliers after transforming them
for (column in still_outliers) {
  distancia_outliers(column)
}

```

We see that in all cases, in terms of standard deviations, the maximum distance of the highest outlier from the mean has decreased. Therefore, the transformation in these cases also has the desired effect, reducing the extremity of the outliers. Consequently, we will maintain this transformation in all the variables where it has been applied, as it will help ensure that the influence of extreme values on the predictions of the model we are going to create is not as high.

## 3.5 - Standardization of variables.

Now we will standardize the quantitative variables of this dataset using the "min-max scaller" technique.

```{r}
# For this task we will use a function
minmax_scaling <- function(column) { # The function has as input a column
  # We get the maximum and the minimum of the column
  min_val <- min(column)
  max_val <- max(column)
  # We apply the min-max scaller formula to the column
  scaled_column <- (column - min_val) / (max_val - min_val)
  return(scaled_column)} # We return the column standarized

# We create a vector with the names of the remaining variables that we also want to standardize.
rest_scalling_vars <- c("Age", "DailyRate", "DistanceFromHome", "HourlyRate",  "MonthlyRate", "PercentSalaryHike")

# We standardize the variables to which we have applied the square root.
for (name in paste0(vars_outliers, "_scaled")) {
  HR_data[[name]] <- minmax_scaling(HR_data[[name]])
}

# We create new standardized variables with the remaining quantitative variables (the ones we did not transformed using the square root).
for (name in rest_scalling_vars) {
  name_scaled <- paste0(name, "_scaled")
  HR_data[[name_scaled]] <- minmax_scaling(HR_data[[name]])
}
```

## 3.6 - Discretizing variables.

We will also create a discretized version of each of these variables that we have standardized. This way, the final dataset will contain multiple versions of the variables, providing more options when testing which variable selection and formats work best for building a model that functions correctly for our purposes. By having variables in multiple formats, we can test different models and introduce various format combinations during training. This will allow us to see which variables and formats work best for the model we ultimately choose. We will use 'k-means' to create the intervals, as this method automatically adjusts the intervals for each partition, ensuring that the number of observations within each interval is similar.

```{r, warning=FALSE}
# We will use the package "arules"
library(arules)

# We will create a vector with the names of all the standardized variables. First, we will store in it the names of the variables with outliers, and then the names of those without outliers (we have these names in another vector). We will add "_scaled" at the end of the name of the variables to do the discretization with the standarized variables.

scaled_vars <- paste0(vars_outliers, "_scaled")
scaled_vars <- c(scaled_vars, paste0(rest_scalling_vars, "_scaled"))
# We check all the values in this vector for ensuring that everything is correct
scaled_vars
```

```{r}
# We iterate over these names
for (name in scaled_vars) {
  # We create a new name for the discretized variable by replacing "_scaled" with "_discret"
  disc_name <- gsub("_scaled", "_discret", name)
  # We create a new variable with the discretized data using k-means, with 4 intervals
  HR_data[[disc_name]] <- discretize(HR_data[[name]], method="cluster", breaks = 4)
}
# We check the first values of these variables
head(HR_data[35:62])
```

We can see that the values are numbers with many decimal places. However, in several variables, some of these values repeat because they represent the boundaries of the intervals used for discretization.

# 4 - SINGULAR VALUES DECOMPOSITION.

After finishing with the EDA and cleaning and manipulating the data, we will explore the dimentionality of the data. In this case we will use the tecnique "Singular values decomposition".

```{r}
# We will perform the decomposition using the standardized versions of the variables. First, we create a dataframe for this purpose
X <- HR_data[scaled_vars]

# We convert this dataframe to a matrix
X <- as.matrix(X)

# We perform a singular value decomposition of this matrix using the "svd" function
x_svd <- svd(X)

```

The object obtained by applying 'svd' internally contains 3 elements. Specifically, these elements correspond to the results of performing singular value decomposition. They are as follows: "u", "d", and "v", which hold the information found in the matrices U, S, and V of the singular value decomposition formula:

A=UxSxV^t

where A is the original data matrix, "U" and "V" are the matrices of right and left singular vectors, and S is the singular matrix, where its diagonal contains the singular values. By multiplying these last 3 matrices as shown in the formula, we would obtain the original data.

In "d" we have the elements that would be on the diagonal of the matrix S, that is, the singular values. These values indicate the relative importance of each column in the matrices U and V, and thus in the data. We will graphically display their values."

```{r}
plot(x_svd$d)
```

We see that the first singular value is the most important, with a significant difference compared to the others. The remaining singular values progressively decrease in magnitude.

By squaring these values and expressing them as a proportion of the total, we can determine the percentage of variance in the original data explained by each singular value multiplied by its corresponding right and left singular vectors.

```{r}
prop.table(x_svd$d^2)
```

We see that using only the first column of "u", "v", and the first value of "d", we can explain almost 80% of the variance in the data. Therefore, with a significantly smaller amount of information, we can represent most of the variance in the original data.

The resources consulted for this section are as follows:

- First: https://rpubs.com/reyzaguirre/460513


- Second: https://rpubs.com/skydome20/92492


- Third: documentation of the function "svd"

- Forth: https://www.displayr.com/singular-value-decomposition-in-r/#:~:text=This%20post%20is%20going%20to,tutorial%20and%20discuss%20SVD%20properties

From this point of the project, multiple models (supervised models, unsupervised models, statistical models...)

# 5 - USING A NON SUPERVISED METHOD (HIERARCHICAL CLUSTERING).

We will use an unsupervised method of **Hierarchical Clustering**, specifically an agglomerative hierarchical clustering method. For this, the following guide (is a guide in Spanish) has been consulted:

https://rpubs.com/mjimcua/clustering-jerarquico-en-r

Previously, we prepared the dataset so that the quantitative variables have both a discrete version and a normalized version using min-max scaling. However, after testing the model, the results obtained when creating it with the variables transformed in this way are very poor. Therefore, two models will be built in parallel, one using normalization with the min-max scaling method and another using standardization, which will result in variables with a mean of 0 and a standard deviation of 1. This will allow us to see the difference in the algorithm's results depending on the measurement scale used.

We will not use the qualitative variables, as we do not know the distance between each of their categories. Since we are using fewer variables than those in the dataset (because not all of them are quantitative), we will take an exploratory approach to see if, with these variables, the observations group into well-defined clusters or on the contrary, a more thorough approach is needed in terms of variable selection based to find clearer groupings.

We start preparing the datasets:

```{r}
# We create a dataset which only contains the normalized variables. We will select them by their indexes (I verifyed them manually)
dat_norm <- data.frame(HR_data[35:48])
# We show the first columns
head(dat_norm)

# We also select the variables in their original verison. We will select the indexes for ensuring the same order as in the original dataset (I verifyed them manually)
dat_est <- HR_data[c(17, 19, 26, 27, 29, 30, 31, 32, 1, 4, 6, 11, 18, 22)]
# We standarize these variables
dat_est <- data.frame(scale(dat_est))
# We show the first columns
head(dat_est)
```

## 5.1 - Model creation.

Now we begin to create the model. Since we are going to create 2 models in parallel, each of the commands used will be repeated twice to store the result with each measurement scale used.

```{r}
par(mfrow=c(1,2))
# We need the library "cluster"
library(cluster)

# We calculate the dissimilarities matrix
d_norm_euc <- dist(dat_norm, method = "euclidean")
d_est_euc <- dist(dat_est, method = "euclidean")

# We create a hierarchical clustering models with each of these distances
hc_norm_euc <- hclust(d_norm_euc, method = "complete")
hc_est_euc <- hclust(d_est_euc, method = "complete")
```

Now we will show the resulting dendrogram from creating this model. Since these dendrograms have many labels, and there's no way for them to be legible (they overlap), I have decided to make the labels on the X-axis very small, and we will focus more on the shape of the obtained clusters.

```{r}
plot(hc_norm_euc, cex = 0.01, hang = -1, main="Normalized variables")
plot(hc_est_euc, cex = 0.01, hang = -1, main="Standarized variables")
```

We see how there are many branches in both cases, but their shapes change depending on the scale of the variables. We didn't set the number of clusters, as this depends on where we cut the tree. This will determine how many branches there are. Each cut branch will be a different cluster. In the next step, we will determine the number of clusters that yield the most appropriate result.

## 5.2 - Analysis of the results of the model.

We will create a function that returns a vector with the medium silhouette of the model, with diffrent number of clusters (the function will test mutiple number of clusters in each model and get the medium silhouette for each case)

```{r}
calculate_silhouettes <- function(model, distances) {
  # We create a vector with 10 zeros that we will replace with the silhouette values
  results <- rep(0, 10)
  
  # We iterate from 2 to 10
  for (i in 2:10) {
    # In each iteration we will cut the dendrogram for getting "i" clusters
    y_cluster <- cutree(model, k = i)
    # We calculate the medium silhouette of the clusters obtaine with this cut of the dendrogram
    sk <- silhouette(y_cluster, distances)
    results[i] <- mean(sk[, 3])
  }

  # Return the results
  return(results)
}

# We call this function with both models previously created and both dissimilarities matrixes calculated
results_1 <- calculate_silhouettes(hc_norm_euc, d_norm_euc)
results_2 <- calculate_silhouettes(hc_est_euc, d_est_euc)

# We show graphically the medium silhouette for each model and each number of clusters
par(mfrow=c(1,2))
plot(2:10,results_1[2:10],type="o",col="blue",pch=0,xlab="Number of clusters",ylab="Silhouette", main = "Using min-max scalling with range 0 to 1")
plot(2:10,results_2[2:10],type="o",col="blue",pch=0,xlab="Number of clusters",ylab="Silhouette", main = "Using normalization.
Mean = 0 ; Standard Deviation = 1")
```

We see that the highest average silhouette score is obtained with 2 clusters using both scaling methods. However, when using standardized distances based on the mean and standard deviation, the result is much better (the Y-axis values in this second case are higher).

Next, we will cut the dendrogram of the best model to obtain this number of clusters (in the model created with standardized distances).

```{r}
plot(as.dendrogram(hc_norm_euc))
rect.hclust(hc_norm_euc, k = 2, border = 3:4)
```

## 5.3 - Conclusions.

Depending on the scale of the variables, the effectiveness of the model changes. Although with both scales the number of clusters with the highest average silhouette is 2, when we standardize the data, the average silhouette is higher than when we normalize it. For this reason, the best model is the 2-cluster model obtained with standardized data. However, we should keep in mind that the maximum average silhouette obtained using Euclidean distance is 0.3. Thus, with this distance metric, the model does not seem to be producing very solid clusters.

It is also important to consider that since we are only using quantitative variables (as distance-based models cannot use categorical variables unless we know the distances between categories), we might be missing information that could be very important for differentiating groups in the data. However, this is a characteristic of the model; since it operates based on distances, there is nothing we can do to address this problem with this data.

# 6 - USING DBSCAN AND OPTICS.

## 6.1 - Creating the model.

This time we will apply DBSCAN and OPTICS only to the dataset that previously gave us the best results, i.e., with the standardized variables. We will follow this working logic:

We will select the 'minPts' values considering the total size of our sample of 1470 observations. We will use a 'minPts' value of 100 to investigate the algorithm's behavior with a small fraction (approximately 7%) of the sample, which represents a lower neighborhood density threshold. In contrast, we will also choose a 'minPts' value of 400, close to 29% of the sample, to examine how the algorithm responds to a significantly higher neighborhood density. The choice of these two extremes aims to provide a clear understanding of how variation in neighborhood density affects the clustering results.

```{r, warning=FALSE}
library(dbscan)
# We train OPTICS algorithm using the default value of eps and the 2 mentioned values of minPts
optics_100 <- optics(dat_est, minPts = 100)
optics_400 <- optics(dat_est, minPts = 400)
```

To train the DBSCAN algorithm, I iteratively adjusted the "eps_cl" values before choosing the ones that will be shown in the code below. I selected values that allow for easy visualization of the behavior of both models.

```{r}
#We extract the agrupation using: eps_cl = 2
dbscan100_2 <- extractDBSCAN(optics_100, eps_cl = 2)
dbscan400_2 <- extractDBSCAN(optics_400, eps_cl = 2)
#We extract the agrupation using: eps_cl = 2.5
dbscan100_2_5 <- extractDBSCAN(optics_100, eps_cl = 2.5)
dbscan400_2_5 <- extractDBSCAN(optics_400, eps_cl = 2.5)
#We extract the agrupation using: eps_cl = 3
dbscan100_3 <- extractDBSCAN(optics_100, eps_cl = 3)
dbscan400_3 <- extractDBSCAN(optics_400, eps_cl = 3)
#We extract the agrupation using: eps_cl = 3.5
dbscan100_3_5 <- extractDBSCAN(optics_100, eps_cl = 3.5)
dbscan400_3_5 <- extractDBSCAN(optics_400, eps_cl = 3.5)
```

Now, for inspecting the results we just need to access the objects we saved and display their results.

## 6.2 - Inspecting the results.

```{r}
# We show the results of boths models trained with OPTICS
optics_100
optics_400
```

We observe that with a significant increase in the neighborhood criterion (minPts), the value of "eps" has increased only slightly. In the first model, it is 7.327, and in the second model, it is 8.8934 (an increase of only a small percentage between 10-20%, despite the neighborhood criterion being multiplied by 4). This means that to find 4 times more points, the search radius only needs to increase slightly.

Now we show graphically the results of these 2 models:

```{r}
par(mfrow=c(1,2))
plot(optics_100, ylim=c(0, 5))
plot(optics_400, ylim=c(0, 5))
```

We can see how the diffrence between both results is not very big. The unique diffrence is that in the first model, the distance for each observation increases in a more progressive way than in the second model. However, the distances on both models are big.

Now we will show the results that dbscan obtains with each of the tested eps_cl values. We will show in paralell the results of each model with each eps_cl value tested. On the right will be the model which used 100 minpts, and on the left the model that used 400.

**eps_cl = 2**

```{r}
par(mfrow=c(1,2))
plot(dbscan100_2, ylim=c(0,5))
plot(dbscan400_2, ylim=c(0,5))
```

Initially, no clusters are detected in either of the models.


**eps_cl = 2,5**

```{r}
par(mfrow=c(1,2))
plot(dbscan100_2_5, ylim=c(0,5))
plot(dbscan400_2_5, ylim=c(0,5))
```

A cluster appears in the first model, but in the second everything is still noise. This outcome is logicical, given that in the first optics model, the initial reachability distances are shorter than in the second optics model.

**eps_cl = 3**

```{r}
par(mfrow=c(1,2))
plot(dbscan100_3, ylim=c(0,5))
plot(dbscan400_3, ylim=c(0,5))
```

The size of the cluster in the first model has increased, and in the second model everything is still noise.

**eps_cl = 3.5**

```{r}
par(mfrow=c(1,2))
plot(dbscan100_3_5, ylim=c(0,5))
plot(dbscan400_3_5, ylim=c(0,5))
```

The cluster in the first model has grown even more and now occupies almost all the observations. Additionally, in the second model, a cluster of nearly the same size as the cluster in the first model has abruptly appeared.

The results reveal interesting patterns. With a relatively low minPts value (100), a cluster gradually forms as the eps_cl parameter increases, indicating more inclusive clustering as the neighborhood distance expands. On the other hand, when minPts is increased to 400, only noise is initially identified, reflecting significant rigidity in neighborhood definition due to the high density required to form a cluster. However, when eps_cl is raised to 3.5, a clustering pattern similar to that of the model with a lower minPts value emerges. This suggests that the data possess an underlying dense structure that only becomes evident under broader distance thresholds.

## 6.3 - Getting a measure of the quality of the clustering.

First, we will examine the appearance of these 2 clusters representing them with 2 principal components (to have a representation in 2 dimensions). We will to this visualization with the 2 last models.

```{r}
par(mfrow=c(1,2))
hullplot(dat_est, dbscan100_3_5)
hullplot(dat_est, dbscan400_3_5)
```

We can see that there are not multiple structure of points. Instead, there in both models there is only a single cluster of points, with some points gradually moving away from this structure. Both models capture a large portion of the points with the highest density and exclude most of the points with lower density. Therefore, it seems logical that the model does not find more than a single cluster.

Since we only have one cluster, we will evaluate its quality based on the "Sum of Squares Within" index. We will do this only with the model whose optics value was 400 before applying DBSCAN. We will calculate the index and its average value for both the points inside the cluster and those outside it, and then compare them to determine if the obtained cluster is dense or not.

```{r}
# We extract the data which is part of the cluster
cluster_data <- dat_est[dbscan400_3_5$cluster == 1, ]

# Whe also extract the data which is not part of the cluster
non_cluster_data <- dat_est[dbscan100_3_5$cluster == 0, ]

# We calculate the centroid of the principal cluster (the mean value of the values of the cluster)
centroid_cluster <- colMeans(cluster_data)

# We calculate the squared summatory of the points of the cluster
ssw_cluster <- sum(rowSums((cluster_data - centroid_cluster)^2))
# And we get the average value of this sum
average_ssw_cluster <- ssw_cluster / nrow(cluster_data)

# And now we get the sum of squares of the points out of the cluster (from the points to the calculated centroid) 
ssw_non_cluster <- sum(rowSums((non_cluster_data - centroid_cluster)^2))
# We get the average value of this sum
average_ssw_non_cluster <- ssw_non_cluster / nrow(non_cluster_data)


# We show both average values
print(paste0("Inside the cluster: ", average_ssw_cluster))
print(paste0("Outside the cluster: ", average_ssw_non_cluster))
```

We see that the mean value of the sum of squared distances to the centroid differs significantly between the points inside the cluster and those outside it. Therefore, it is clear that the cluster identified by the model is dense (at least denser than everything outside of it). However, is not clear if with this data using a method like this is more useful than applying an outlier detection method.

# 7 - DECISION TREE MODEL.

## 7.1 - Train test split.

**A last data manipulation**

This project is a migration of other project. When I was doing it, I forgot to delete the variable "Over18" until this point of the project. If I delete before in the script, some parts stop working because refactoring is needed. For now, this variable will be eliminated in this point of the project. The proper solution will be to delete it in the part of the script where all de variable elimination are done and refactor the code. But for now, we keep this solution.

```{r}
# We delete the variable "Over18"
name <- "Over18"
index <- match(name, names(HR_data))
HR_data <- HR_data[-index]



# TO DO

# THIS VARIABLE SHOULD BE ELMINATED IN THE SECTION WHERE ALL THE ELIMINATIONS ARE MADE, AND REFACTOR THE CODE. ALSO IS REQUIERED DELETING THIS CODE AFTER REFACTORING
```

**Continuation with the project**

Before starting with the selection of the training and test samples, we are going to choose only the variables with which we will build the model. We will consider 2 things for this; first, we will not select more than one version of the the same variable, and second, we will choose only the most important/relevant ones for building our model.

The discrete/standardized versions of some variables that we previouslty created are at the end of the dataset, from variable 34 onwards. Therefore, we will only select the first 33 to build the model. Since a rule-based model requires understanding the meaning of the variables, we will choose the original variables, to improve the interpretability of the model.

```{r}
# We select all the original variables.
split_dat <- HR_data[1:33]
# Previously we also created some new variables, based on the value of others. For creating the decision tree we will only use the new variables, so we will discard the variables used for creating them.

# We save in a vector their names
discard_vars <- c("JobSatisfaction", "EnvironmentSatisfaction", "RelationshipSatisfaction")
# We discard these variables
indexes_eliminate <- match(discard_vars, colnames(split_dat))
split_dat <- split_dat[-indexes_eliminate]
# Mostramos cuantas columnas nos quedan en el dataset
ncol(split_dat)
```

We still have a lot of variables. We will select only the most important ones, because having too many variables increases the risk of including irrelevant variables in the training, which can also have an impact on the predictions.

```{r, warning=FALSE}
# We load the required packages for doing the study of the importance of the variables
library(RWekajars)
library(RWeka)
library(FSelector)

# We create a dataframe to store the results of the evaluation of each variable's importance
results_df <- data.frame()

# First we iterate over each column of the dataset
for (var in colnames(split_dat[-2])) {
  # We create a formula which includes the variable "Attrition" and the iterated variable, that will have to format "Attrition ~ var"
  formula <- as.formula(paste("Attrition ~", var))
  # We store in a variable the information gain of each iterated variable
  info_gain <- information.gain(formula, split_dat)
  # We add the result to the dataframe with the results
  results_df <- rbind(results_df, info_gain)
}


# Now we sho the final datarame sorting the rows by the values of the column created for measuring the importance of the variable
results_df <- data.frame(results_df[order(results_df$attr_importance),,drop=FALSE])
results_df
```

Some variables have very low importance. We will use a threshold of 0.005 for using a variable to train the model. Any variable with an importance lower than that value will be discarded.

```{r}
# We save in a vector the names of the variables not matching the previous condition
less_importance <- row.names(results_df)[1:14]
# We discard them
indexes_to_eliminate <- match(less_importance, colnames(split_dat))
split_dat <- split_dat[-indexes_to_eliminate]
```

Now we will define a parameter that will control the splitting of the dataset.

```{r}
set.seed(7)
# The proportion of the data for the test dataset will be 1/5
split_prop <- 5
# We define the indexes of the dataset for the test dynamically
indexes <- sample(1:nrow(split_dat), size=floor(((split_prop-1)/split_prop)*nrow(split_dat)))

# We select the rows corresponding to these indices for the training set, and all indices that are not in the indexes vector for the test set.
train_dat <- split_dat[indexes,]
test_dat <- split_dat[-indexes,]
```

We used a proportion of 1/5 for splitting the dataset into test and train sets, because the size of the dataset is not too small (in that case, using a similar length for both datasets would be better), but the size is not too big (if it had hundreds of thousands of observations, we could have used a smaller proportion for the split).

## 7.2 - Training the model.

Our target variable is the variable **Attrition**, if we read the dictionary that we have created at the beginning of the project, we can see that this variable indicates whether an employee has left the organization.

We will create a decision tree model for predicting this variable.

```{r, warning=FALSE}
set.seed(7)
# We load the C50 library, which implements a decision tree model
library(C50)
# We save inside a vector the names of all trainig variables (except "Attrition")
columns_for_model <- setdiff(names(train_dat), "Attrition")
# We create a rules based model, initially using the default algorithm (without any parameter)
defalut_model <- C5.0(train_dat[columns_for_model], train_dat$Attrition, rules=TRUE)
# We save a summary of the model
sumary_model <- summary(defalut_model)
# Before showing the model, we will show a frequency table of the target variable
table(train_dat$Attrition)
```

We see that the "1" category (the employee will leave his job) appears much less frequently than the "0" category. We will consider this when analysing the results of the model.

```{r}
sumary_model
```

We obtained a model with 17 rules, 6 applying for class 0 (the person didn't leave the organisation), and 11 rules for the class 1 (the person left the organisation). The rules for the class 0 are more generic (the conditions are not so complex) and apply to many more cases than the rules for the class 1. On the other hand, while some rules of the class 1 are also somewhat general, others are very specific, and apply to a very small number of cases. This indicates that the model is possibly overfitted for the class 1, because the rules could be generated based on very specific cases that may be present only in this dataset.

Now we will display graphically the model using the same configuration.

```{r fig.width=13, fig.height=6,dpi=300}
set.seed(7)
# We load "grid" package for manipulating some graphical parameters
library(grid)
# We show graphically the model
model_plot <- C5.0(train_dat[columns_for_model], train_dat$Attrition)
plot(model_plot, gp=gpar(fontsize=7))
```

We see how the tree is quite complex, with multiple branches, and it is quite deep. The nodes and texts within it are generally legible, but the complexity of the tree gives it an appearance that is overloaded with information.

Now, for a fast evaluation of this model (that is only an initia model) we will create a confusion matrix.

```{r}
# We predict the results of the model in the training dataset
prediction <- predict(defalut_model, test_dat[columns_for_model])
# We calculate the accuracy of the model in the test dataset
100*sum(prediction==test_dat$Attrition)/length(prediction)
# We will use "crosstable" function from the package "gmodels" for creating the model
library(gmodels)
CrossTable(test_dat$Attrition, prediction, prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE, dnn = c('Reality', 'Prediction'))
```

The accuracy of the model is relatively high, however, if we observe the confusion matrix, the model doesn't predict very well the class "1" (the employee left the organisation). For this class we had a lot of very specific rules, but, those rules probably were very specific to the train tataset, and do not apply to other cases.

We will try to solve this problem using pruning options while training the model, this will avoid creating overly complex ramifications, and will lead to more general rules that will likely be useful in various datasets.

```{r}
prunned_model <- C5.0(train_dat[columns_for_model], train_dat$Attrition, rules=TRUE, control = C5.0Control(noGlobalPruning = FALSE, CF = 0.1))

prunned_model
```

The number of rules has decreased from 17 to 8.

```{r}
summary(prunned_model)
```

The number of very specific rules has decreased. However, the rule that applied to the smallest number of cases in the last model, is still present here (Rule 5: 	JobRole = Human Resources;	TotalWorkingYears <= 2). If we wanted, we could continue refining this model to find a better one. However, in this practical case we will stop here to avoid extending it excessively.

We will show graphically this model, for understanding visualy how it simplifyed.

```{r fig.width=13, fig.height=6,dpi=300}
prunned_model_plot <- C5.0(train_dat[columns_for_model], train_dat$Attrition, control = C5.0Control(noGlobalPruning = FALSE, CF = 0.1))

plot(prunned_model_plot, gp=gpar(fontsize=12))
```

Now we can see that the model is much more understandable than before. If this model is also better (or at least equal) for predicting both classes, we will be able to say that pruning it was something positive.

We will now evaluate this model more thoroughly than the previous one.

## 7.3 - Evaluating the model.

For the evaluation we will create a function that will return all the metrics that we can obtain for evaluating supervised models.

```{r}
# The input of the function are all the values from the confusion matrix
calcuateMetrics <- function(TP, TN, FP, FN) {
    # The function will calculate all the measures used for evaluating supervised models
    accuracy <- (TP + TN) / (TP + TN + FP + FN)
    specificity <- TN / (TN + FP)
    presicion <- TP / (TP + FP)
    sensitivity <- TP / (TP + FN)
    F_Measure <- 2 * ((presicion * sensitivity) / (presicion + sensitivity))
    
    # We print a formatted text showing all of these values
    cat(paste0("Accuracy: ", round(accuracy, 2), "\n",
               "Specificity: ", round(specificity, 2), "\n",
               "Presicion: ", round(presicion, 2), "\n",
               "Sensitivity: ", round(sensitivity, 2), "\n",
               "F-Measure: ", round(F_Measure, 2), "\n"))
}
```

Now we will use this function with the confusion matrix of this model.

```{r}
# We make predictions with the pruned model
prediction_pruned <- predict(prunned_model, test_dat[columns_for_model])
# We show the confusion matrix
CrossTable(test_dat$Attrition, prediction_pruned, prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE, dnn = c('Reality', 'Prediction'))
```

First of all, if we compare this confusion matrix with the previous one, the results of this model is slightly worse, because this model makes some more false positives -22 vs 19 that were before-, but considering that is a much more simple model, we will not consider this as a problem.

Now we use the function.

```{r}
calcuateMetrics(15, 226, 22, 31)
```

With these values, we can extract some conclusions about the model. First, this model has a relatively high **accuracy** (0.81). This model could be useful for getting a general idea about possible outcomes in a company. However, the model is not extremely accurate and will make some mistakes.

The value of the **specificity** (0.91) is higher than the accuracy, which indicates that the model is better for predicting the negative class than the positive. On the other hand, the low values of the **precision** (0.41) and the **sensitivity** (0.33) are indicating that the model is not very good for detecting the positive class.

This interpretation is confirmed by the value of the **F-Measure** (0.36), which is low. This indicates that the model has not a good balance between the number of cases correctly detected of each class (having a worse capacity for detecting the negative class).

These results represent a limitation for the situations where the model can be used. For specific detections of the positive class (i.e. determining which case will be positive), this model is contraindicated as could make mistakes, and lead to discrimination in the company that uses the model this way. However, this model could be useful for descriptive situations where the goal is to identify situations where the risk of attrition is low or high.

## 7.4 - Posible improvements for the development of a better model.

Given that this model has a low value in the F-Measure, is possible to improve it, and having better general results for predicting both classes (and even, if the precision were much higher, it would be possible to use this model for more specific situations and not only for general descriptions).

The principal problem may stem from the unbalanced classes (there are much more cases of the negative than of the positive). Balancing them is mandatory for a better performance, some techniques for creating synthetic observation should be used, such as SMOTE (Synthetic Minority Over Sampling Technique) or ADASYN (Adaptive Synthetic Sampling Approach).

Additionaly, a more thorough tuning of the model parameters model should be done. Using a grid search strategy with multiple combinations of values of the parameters of the model, combined with creating new synthetic observations, would possibly bring to find the best model for this data. If this strategy is not bringing significantly better models, new data shold be collected, specially about the positive class.